{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/michalis0/Business-Intelligence-and-Analytics/blob/master/labs/08%20-%20Regression2/walkthrough/walkthrough_08.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u461z_Wk-K-P"
      },
      "source": [
        "<h1 align=\"center\"> WALKTHROUGH 8</h1>\n",
        "\n",
        "<div>\n",
        "<td> \n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Logo_Universit%C3%A9_de_Lausanne.svg/2000px-Logo_Universit%C3%A9_de_Lausanne.svg.png\" style=\"padding-right:10px;width:240px;float:left\"/></td>\n",
        "<h2 style=\"white-space: nowrap\">Business Intelligence and Analytics</h2></td>\n",
        "<hr style=\"clear:both\">\n",
        "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
        "\n",
        "</div>\n",
        "\n",
        "This week we continue working on regression. Diving into more specific methods, we will show you how to choose the number of parameters, cross validation and 1-hot/label encoding."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gEjK31c_tCl1"
      },
      "source": [
        "# REGRESSION WITH CATEGORICAL VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi8vCkNF_B3P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "%matplotlib inline\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "32vbaIBt_5KF"
      },
      "source": [
        "## LOAD THE DATASET\n",
        "\n",
        "The Dataset we will be working on was extracted and modified from [Kaggle](https://www.kaggle.com/mohansacharya/graduate-admissions). It gives us various information concerning students and their chances of admission.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "FRP6bS5fABwY",
        "outputId": "09e6dff5-5e62-4c79-ac15-20eb20044f20"
      },
      "outputs": [],
      "source": [
        "#Load the dataset\n",
        "url = 'https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/master/data/Admissions_prediction.csv'\n",
        "GAD = pd.read_csv(url, sep = \";\", index_col= 'Serial No.').drop_duplicates() #Graduate Admissions Data\n",
        "\n",
        "display(GAD.head())\n",
        "print(GAD.dtypes)\n",
        "print(\"Data matrix shape: \", GAD.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRv1WpGd_fXY"
      },
      "source": [
        "An overview of the columns:\n",
        "\n",
        " \n",
        " * `GRE Score`: GRE is a standardized admission test (out of 340)\n",
        " * `TOEFL Score`: English knowledge (out of 120)\n",
        " * `SOP`: Standard of Purpose (out of 5)\n",
        " * `LOR`: Letter of Recomendation (out of 5)\n",
        " * `CGPA`: College GPA (out of 10)\n",
        " * `RESEARCH`: Whether the applicant did research or not\n",
        " \n",
        " We will try to predict the chance of admit with the other variables."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BU0MR4S4_9k9"
      },
      "source": [
        "## HANDLING CATEGORICAL VARIABLES FOR REGRESSION\n",
        "First, let's focus on the `Research` column. The applicant was either active in research, or he was not. There are therefore only 2 possible values, hence we will assign 1 for YES and 0 for NO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2kOpYqBAVOW"
      },
      "outputs": [],
      "source": [
        "GAD['Research'] = GAD['Research'].replace(['NO', 'YES'], [0,1])\n",
        "display(GAD.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qdFhlJ_psym"
      },
      "source": [
        "Here let's get a quick overview of the existing correlations between our variables using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "nERSyNFIdKYU",
        "outputId": "3f199803-9548-42a4-ff22-c1d08ae71deb"
      },
      "outputs": [],
      "source": [
        "d = sns.heatmap(GAD.corr(numeric_only=True), annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZ47n4bO6J9"
      },
      "outputs": [],
      "source": [
        "# We want to predict the chance of admission\n",
        "y = GAD[['Chance of Admit']]\n",
        "\n",
        "# With the help of the other columns\n",
        "X = GAD.drop('Chance of Admit', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmtpafMDSjep"
      },
      "source": [
        "Now we need to handle the `University Ranking`. \n",
        "We will do both, one-hot and label encoding.\n",
        "For label encoding, we can use Sklearn's LabelEncoder() function. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMz58tDKSliV"
      },
      "outputs": [],
      "source": [
        "# Label encoder\n",
        "X_label = X.apply(LabelEncoder().fit_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "0wVLnTrdqf88",
        "outputId": "d68d4b68-b041-4049-8c89-8896596ca045"
      },
      "outputs": [],
      "source": [
        "display(X_label.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlj_DnDMqr_z"
      },
      "source": [
        "For one hot encoding we use pandas get dummy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "IyuTZwch3Vbs",
        "outputId": "407afd71-9e50-4ae5-8d37-975ddd956ff5"
      },
      "outputs": [],
      "source": [
        "# 1-hot encoding\n",
        "\n",
        "# We create a DF with Dummy variables\n",
        "dummies = pd.get_dummies(GAD[\"University Rating\"])\n",
        "X_hot = pd.concat([GAD, dummies], axis = 1)\n",
        "\n",
        "# We drop the Rating column \n",
        "del X_hot[\"University Rating\"]\n",
        "del X_hot[\"Chance of Admit\"]\n",
        "\n",
        "# Have a look at what dummies actually looks like\n",
        "dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "QNlYISNDqWc7",
        "outputId": "1f795b32-a399-478d-b2c2-8186a32c6135"
      },
      "outputs": [],
      "source": [
        "display(X_hot.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQK3FiQe-lll"
      },
      "source": [
        "We will now build two models, one for 1-hot encoding and one for label encoding.\n",
        "Let's start with label encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6IAkPe5_GlF",
        "outputId": "f6071b51-0d09-4dcf-81c1-2af1fa9eded7"
      },
      "outputs": [],
      "source": [
        "# Split Dataset\n",
        "X_train_lab, X_test_lab, y_train_lab, y_test_lab = train_test_split(X_label, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "\n",
        "# Fit the model\n",
        "model_lab = LinearRegression()\n",
        "model_lab.fit(X_train_lab, y_train_lab)\n",
        "\n",
        "# Calculate R2\n",
        "lab_r2 = round(model_lab.score(X_train_lab, y_train_lab), 4)\n",
        "print(\"R^2 of train set using label encoding: \", lab_r2 )\n",
        "\n",
        "# Calculate Scores on Test-Set\n",
        "label_predictions = model_lab.predict(X_test_lab)\n",
        "lab_mae = mean_absolute_error(y_test_lab, label_predictions)\n",
        "lab_mse = mean_squared_error(y_test_lab, label_predictions)\n",
        "lab_r2_test = r2_score(y_test_lab, label_predictions)\n",
        "\n",
        "print(\"MAE LAB %.2f\" % lab_mae)\n",
        "print(\"MSE LAB %.2f\" % lab_mse)\n",
        "print(\"R^2 score LAB %.4f\" % lab_r2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CdawFAZDMgM"
      },
      "source": [
        "And continue with 1-hot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVFRDpP9-1VQ",
        "outputId": "7e03d721-24e1-4a08-eded-16446abfcd17"
      },
      "outputs": [],
      "source": [
        "# Split Dataset\n",
        "X_train_hot, X_test_hot, y_train_hot, y_test_hot = train_test_split(X_hot, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "\n",
        "# Fit the model\n",
        "model_hot = LinearRegression()\n",
        "model_hot.fit(X_train_hot, y_train_hot)\n",
        "\n",
        "# Calculate R2\n",
        "hot_r2 = round(model_hot.score(X_train_hot, y_train_hot), 4)\n",
        "print(\"R^2 Train Score using 1-hot encoding : \", hot_r2 )\n",
        "\n",
        "# Calculate Scores on Test-Set\n",
        "hot_predictions = model_hot.predict(X_test_hot)\n",
        "hot_mae = mean_absolute_error(y_test_hot, hot_predictions)\n",
        "hot_mse = mean_squared_error(y_test_hot, hot_predictions)\n",
        "hot_r2_test = r2_score(y_test_hot, hot_predictions)\n",
        "\n",
        "print(\"MAE HOT %.2f\" % hot_mae)\n",
        "print(\"MSE HOT %.2f\" % hot_mse)\n",
        "print(\"R^2 score HOT %.4f\" % hot_r2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-pi4KJa_x39",
        "outputId": "2a7e1779-be06-4382-e919-80c8c3d9bde3"
      },
      "outputs": [],
      "source": [
        "print(\"The 1-hot encoding method yields an R^2 {}% higher than using label encoding for the train set.\".format(round((hot_r2/lab_r2 -1)*100, 3)))\n",
        "\n",
        "print(\"The 1-hot encoding method yields an R^2 {}% higher than using label encoding for the test set.\".format(round((hot_r2_test/lab_r2_test -1)*100, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC7JH-e0Dd4d"
      },
      "source": [
        "Have a look at how the coefficients change for each method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAMS6qKIHC35",
        "outputId": "cd11d2a7-1df0-4bec-da78-0cab1728a9e7"
      },
      "outputs": [],
      "source": [
        "print(\"Intercept: \", model_hot.intercept_[0]) \n",
        "print(\"Intercept: \", model_lab.intercept_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEJzhlGNIR0E",
        "outputId": "e907781e-b317-48fc-b601-0c105de38d8e"
      },
      "outputs": [],
      "source": [
        "weights_hot = model_hot.coef_.flatten()\n",
        "weights_lab = model_lab.coef_.flatten()\n",
        "\n",
        "print(\"Features coefficients for HOT (weigths): \", model_hot.coef_)\n",
        "print(\"Features coefficients for LAB (weigths): \", model_lab.coef_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HmwVCD3rWxfT"
      },
      "source": [
        "# POLYNOMIAL LINEAR REGRESSION\n",
        "We will now have a look at another model. We are going to use `Advertisement` data we previously used. The task is to figure out how different means of advertisement influence the amount of sales of a product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "s-2cmk3BXdcJ",
        "outputId": "984c1e5a-2314-4b23-f065-7c783b112b01"
      },
      "outputs": [],
      "source": [
        "ad_df = pd.read_csv('https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/master/data/Advertising.csv')\n",
        "# view the first 5 rows \n",
        "ad_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m4OAEl1X2f5"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for the regression\n",
        "y = ad_df[\"Sales\"]\n",
        "X = ad_df.drop(\"Sales\", axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dB3hlTFYgOx",
        "outputId": "7c3f4e02-cc40-46a1-e044-6248c8e58f39"
      },
      "outputs": [],
      "source": [
        "# First let's go for a linear multivariate regression\n",
        "# You should know the drill by now\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Performance metrics\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Test scores: \")\n",
        "print(\"MAE %.2f\" % mae)\n",
        "print(\"MSE %.2f\" % mse)\n",
        "print(\"R^2 %.2f\" % r2)\n",
        "\n",
        "print(\"Train metrics\")\n",
        "print(\"params: \", model.coef_)\n",
        "print(\"constant: \", model.intercept_)\n",
        "print(\"R^2 score: \", model.score(X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vat_JVqHsm7B"
      },
      "source": [
        "Using polynomial regression enables you to predict the best fit line that follows the pattern(curve) of the data. It tends to increase the performance of the model. The function PolynomialFeatures generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. Let's try it on our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkr3ptZwaVa4"
      },
      "outputs": [],
      "source": [
        "# Polynomial regression\n",
        "poly = PolynomialFeatures(2)\n",
        "X = np.array(ad_df[[\"TV\", \"Radio\"]])\n",
        "y = np.array(ad_df[\"Sales\"])\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "LR = LinearRegression(fit_intercept=False) # We don't need fit intercept sice polynomial features function add a column of ones to the data \n",
        "LR.fit(X_poly, y)\n",
        "print(\"params: \", LR.coef_)\n",
        "print(\"R^2 score: \", LR.score(X_poly, y))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RAXvuEElrTSF"
      },
      "source": [
        "## OVERFITTING\n",
        "Beware, adding too many features may cause overfitting.\n",
        "Remember that overfitting is is the tendency of data mining procedures to tailor models to the training data, at the expense of generalization to previously unseen data points. We want to avoid this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "-_KqypF0rV_7",
        "outputId": "fd44a81c-3657-458b-c412-2804524d7867"
      },
      "outputs": [],
      "source": [
        "train_err = []\n",
        "test_err = []\n",
        "for f in range(1,7):\n",
        "    poly = PolynomialFeatures(f)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=12)\n",
        "    LR = LinearRegression(fit_intercept=False)\n",
        "    LR.fit(X_train, y_train)\n",
        "    train_err.append(mean_squared_error(y_train, LR.predict(X_train)))\n",
        "    test_err.append(mean_squared_error(y_test, LR.predict(X_test)))\n",
        "\n",
        "\n",
        "fig=plt.figure(figsize=(11,7))\n",
        "plt.plot(range(1,7), train_err, label=\"train_error\")\n",
        "plt.plot(range(1,7), test_err, label=\"test_error\")\n",
        "plt.legend(fontsize=10)\n",
        "plt.xlabel(\"polynomial degree\")\n",
        "dummy = plt.ylabel(\"error\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_cRswO1mo1j",
        "outputId": "9213a752-4c4e-464e-bb91-ccf21ddde711"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "y = ad_df[\"Sales\"]\n",
        "X = ad_df.drop(\"Sales\", axis = 1)\n",
        "\n",
        "kf = KFold(n_splits = 10, random_state = None)\n",
        "model = LinearRegression()\n",
        "\n",
        "mae_cumm = []\n",
        "mse_cumm = []\n",
        "r2_cumm = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "  \n",
        "  X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "\n",
        "  mae_cumm.append(mae)\n",
        "  mse_cumm.append(mse)\n",
        "  r2_cumm.append(r2)\n",
        "\n",
        "  \n",
        "mean_mae = sum(mae_cumm)/len(mae_cumm)\n",
        "mean_mse = sum(mse_cumm)/len(mse_cumm)\n",
        "mean_r2 = sum(r2_cumm)/len(r2_cumm)\n",
        "\n",
        "print(\"The mean absolute error of all our folds was: \",round(mean_mae, 3))\n",
        "print(\"The mean squared error of all our folds was: \", round(mean_mse, 3))\n",
        "print(\"The mean accuracy of all our folds was: \", round(mean_r2, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii28pGKIygC-"
      },
      "source": [
        "And for higher order polynomial regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "akT_H53UysOx",
        "outputId": "b89ee489-b839-4e03-dac4-895631a29537"
      },
      "outputs": [],
      "source": [
        "train_err = []\n",
        "test_err = []\n",
        "\n",
        "y = ad_df[\"Sales\"]\n",
        "X = ad_df.drop(\"Sales\", axis = 1)\n",
        "\n",
        "kf = KFold(n_splits = 10, random_state = None)\n",
        "model = LinearRegression(fit_intercept = False)\n",
        "\n",
        "for f in range(1,7):\n",
        "  train = []\n",
        "  test = []\n",
        "  for train_index, test_index in kf.split(X):\n",
        "    poly = PolynomialFeatures(f)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    X_train, X_test = X_poly[train_index,:], X_poly[test_index,:]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    train.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "    test.append(mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "  train_err.append(statistics.mean(train))\n",
        "  test_err.append(statistics.mean(test))\n",
        "\n",
        "fig=plt.figure(figsize=(11,7))\n",
        "plt.plot(range(1,7), train_err, label=\"train_error\")\n",
        "plt.plot(range(1,7), test_err, label=\"test_error\")\n",
        "plt.legend(fontsize=10)\n",
        "plt.xlabel(\"polynomial degree\")\n",
        "plt.ylabel(\"error\")\n",
        "print(train_err)\n",
        "print(test_err)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Walkthrough_Regression_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0cc462c96df3621bcc58e01fadcdf9264a069c5c4bbf07201077bb349d3c6bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
