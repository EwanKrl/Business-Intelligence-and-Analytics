{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/Business-Intelligence-and-Analytics/blob/Editing/labs/02%20-%20Pandas%20and%20Python/walkthroughs/walkthrough_pandas_load_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k_qF-UHvc3B"
      },
      "source": [
        "<h1 align=\"center\"> Walkthrough - Lab 2</h1>\n",
        "\n",
        "<div>\n",
        "<td>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Logo_Universit%C3%A9_de_Lausanne.svg/2000px-Logo_Universit%C3%A9_de_Lausanne.svg.png\" style=\"padding-right:10px;width:240px;float:left\"/></td>\n",
        "<h2 style=\"white-space: nowrap\">Business Intelligence and Analytics</h2></td>\n",
        "<hr style=\"clear:both\">\n",
        "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT7WDJuNMc-F"
      },
      "source": [
        "# Basic Pandas operations\n",
        "\n",
        "**Goal**: Our goal here is to learn how to load a dataset into a Pandas DataFrame. The dataset can come either in CSV or in JSON format. We will see also how to perform basic data manipulations and very basic data visualizations so that you understand the nature of your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV2jka2Hvc3F"
      },
      "source": [
        "## 1. Loading a dataset in CSV format\n",
        "\n",
        "First you have to import the `pandas` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z3Tez5xNcE2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # press shift+enter to execute it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3kh8_9rNfLZ"
      },
      "source": [
        "Now you can see that you autocomplete your code with functions that are included in `pandas`. Eg type `pd.read` and see that it recommends some functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Up10t7Mc-H",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# let's load a file\n",
        "url = \"https://media.githubusercontent.com/media/michalis0/Business-Intelligence-and-Analytics/master/data/pandas_tutorial_read.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHgAjE6Mc-O"
      },
      "source": [
        "Is the above correct? Most likely not.\n",
        "\n",
        "CSV stands for \"Comma Separated Values\", which is not the case here. We see there are ';' and the data seem to be in one column. The default delimiter is ',' so we need to change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv5UVka1Mc-Q"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(url, delimiter=';')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2PcFLDEMc-U"
      },
      "source": [
        "This looks better. But something else does not look good now. Our DataFrame is missing a header, let's assign one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p8R3uQFMc-V"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(url,\n",
        "                   delimiter=';',\n",
        "                   names = ['my_datetime', 'event', 'country', 'user_id', 'source', 'topic'])\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwwVIxHfMc-c"
      },
      "source": [
        "With the `head` function you see the first few lines. The .shape function shows the shape of the dataframe (rows, columns). You can also see the:\n",
        "\n",
        "- whole dataset: just type ```data```\n",
        "- the beginning as before ```data.head()```\n",
        "- the last 5 entries ```data.tail()``` or\n",
        "- a sample such as ```data.sample(5)```\n",
        "- some descriptive statistics ```data.describe()```\n",
        "Try it out below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorAv2x0Mc-d"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUv7cCsl_g7T"
      },
      "outputs": [],
      "source": [
        "#Use this cell to write your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6qhq8t3Mc-f"
      },
      "source": [
        "### DataFrame components\n",
        "There are three components of the DataFrame: the index, columns and data (values). We can extract each of these components into their own variables. Let's do that and then inspect them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SssBXmqWMc-f"
      },
      "outputs": [],
      "source": [
        "index = data.index\n",
        "columns = data.columns\n",
        "values = data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcFpF9yoMc-i"
      },
      "outputs": [],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2PH_uMMMc-j"
      },
      "outputs": [],
      "source": [
        "columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV6mB-WBMc-l"
      },
      "outputs": [],
      "source": [
        "values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF-4EXbOMc-n"
      },
      "source": [
        "### Data types of the components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcUBNR9AMc-n"
      },
      "outputs": [],
      "source": [
        "type(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iccy4ZE8Mc-p"
      },
      "outputs": [],
      "source": [
        "type(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDBYWUCrMc-q"
      },
      "outputs": [],
      "source": [
        "type(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihQFsP5DMc-s"
      },
      "source": [
        "\n",
        "The index and the columns are the same type: a pandas **`Index`** object (**`RangeIndex`** is of type **`Index`**), which is a sequence of labels for either the rows or the columns.\n",
        "\n",
        "The values are a NumPy **`ndarray`**, which stands for n-dimensional array, and is the primary container of data in the NumPy library. Pandas is built directly on top of NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPZ-0mD_Mc-s"
      },
      "source": [
        "### Selecting columns\n",
        "\n",
        "If you want to select two particular columns you can do it like that:\n",
        "\n",
        "```data[['country', 'user_id']]```\n",
        "\n",
        "or you can take the columns in a different order:\n",
        "\n",
        "```data[['user_id', 'country']]```.\n",
        "\n",
        "The way to remember the syntax is that outer brackets signify that you want to select columns, and the inner brackets are for the list itself.\n",
        "\n",
        "Try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEQ-RAlMc-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6VtRZPtMc-u"
      },
      "source": [
        "The above returns a pandas.DataFrame. If you want to return a pandas.Series instead then you can use this syntax:\n",
        "\n",
        "```data.user_id ```\n",
        "\n",
        "or\n",
        "\n",
        "``` data['user_id'] ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waGTpqepMc-u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw3hClEJMc-w"
      },
      "source": [
        "If you want to filter one those users that came from SEO then you can write:\n",
        "\n",
        "``` data[data.source == 'SEO'] ```\n",
        "\n",
        "where the inner statement will create a boolean mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzpqqRYtMc-w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJYHORgqMc-x"
      },
      "source": [
        "### Chaining\n",
        "\n",
        "You can combine multiple selection methods as follows:\n",
        "\n",
        "``` data.head()[['country', 'user_id']] ```\n",
        "\n",
        "**CAUTION**: A thing to keep in mind is that when you use chaining you work on *copies* of the original DataFrame. So if you use chaining to change data, you may observe that the original DataFrame was not changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYETQYsOMc-y"
      },
      "source": [
        "#### Exercice 1:\n",
        "Now it's your turn to solve an exercise and deepen your knowledge.\n",
        "\n",
        "Select the user_id, the country and the topic columns for the users who are from country_2 and show only the first 10 results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg4wCaGiMc-z"
      },
      "outputs": [],
      "source": [
        "# enter your solution here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON8xtuzBdCoc"
      },
      "source": [
        "### Column Types\n",
        "\n",
        "Remember that each column/attribute of our data may have different attribute types. Have a look at the following table to understand the different DataTypes in Pandas and Python.\n",
        "\n",
        "| Pandas dtype  | Python type  | NumPy type|Usage\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| object| str or mixed | string_, unicode_, mixed types| Text or mixed numeric and non_numeric values |\n",
        "| int 64| int| int_, int8, int16, int32, int64, uint8, uint18, uint32, uint64 | Integer numbers i.e. [1,2,3,...] |\n",
        "| float64| float| float_, float15, float32, float64 | Floating point numbers (They contain decimal points) |\n",
        "| bool| bool|bool_| True/False values|\n",
        "| datetime64 | NA | datetime64[ns]     | Date and time values  |\n",
        "| timedelta[ns] | NA  | NA| Differences between two datetime|\n",
        "| category | NA| NA| Finite list of text values|\n",
        "\n",
        "Although Pandas will correctly infer the type of data most of the times, it is important to check our columns and convert them if needed. Otherwise our results might get messed up.\n",
        "\n",
        "The function `.dtypes` allows us to see the type of each column. Run the following cell to see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9reNjiHfRSj"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L2tFm-Kfzt6"
      },
      "source": [
        "We can see that altough most columns seem to have the correct type, we can still improve our DataFrame. Let's convert the [\"my_datetime\"] column to the \"datetime64\" type, so that Pandas knows this column contains Date and Time values.\n",
        "\n",
        "For this, the [.to_datetime()](https://https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) function might come in handy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGXheJTNh2gp"
      },
      "outputs": [],
      "source": [
        "# First, let's convert the column type\n",
        "data[\"my_datetime\"] = pd.to_datetime(data[\"my_datetime\"])\n",
        "\n",
        "#Now let's have a look at the dtypes once again\n",
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2agT1bFijyS"
      },
      "source": [
        "The user_id column seems to contain integers. Pandas did not correctly infer it since some of the rows in this column are corrupted (they contain other values than integers).\n",
        "\n",
        "However, we can still use the `pd.to_numeric` function to convert the column to the correct type. We can use the `errors = 'coerce' argument to handle these errors. This will set all cells in the [\"user_id\"] column containing corrupted entries to NaN (Not a Number).\n",
        "\n",
        "Other possible values for \"errors=\" are:\n",
        " * `errors = raise` This will raise an error if it encounters an invalid entry\n",
        " * `errors = ignore` This will return the original data (the data we passed as an argument) if it encounters an error. No conversion will take place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvklRN2uk8tg"
      },
      "outputs": [],
      "source": [
        "#Let's convert the column type\n",
        "data[\"user_id\"] = pd.to_numeric(data[\"user_id\"], errors='coerce')\n",
        "\n",
        "#Now let's have a look at the dtypes once again\n",
        "data.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPH8sxAu8xdT"
      },
      "source": [
        "Since the number of possible countries is limited, we can convert that column to the categorical type with the `.astype()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns2RWHFp9Asl"
      },
      "outputs": [],
      "source": [
        "#Change dtype\n",
        "data['country'] = data['country'].astype('category')\n",
        "\n",
        "#Now let's have a look at the dtypes once again\n",
        "data.dtypes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orVO6jOCyVPd"
      },
      "source": [
        "### Dropping missing values\n",
        "\n",
        "Since our column contained some corrupted data, those cells were set to NaN.\n",
        "The function `.isnull()` will return a DataFrame with values:\n",
        " * False for cells where the data is not null\n",
        " * True for cells where the data is null\n",
        "\n",
        "We can then use `.sum()` to sum all values where the latter is True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gred764ziGW"
      },
      "outputs": [],
      "source": [
        "#Let's see how many column values are missing\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOJDUQuC1Gu2"
      },
      "source": [
        "We can now remove the missing values with the `.dropna()` function.\n",
        "If no argument is passed, this function will drop all rows where at least one of the values is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ENKXDKG1Rv0"
      },
      "outputs": [],
      "source": [
        "#We first drop all the rows\n",
        "data = data.dropna()\n",
        "\n",
        "#Let's have another look\n",
        "data.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtrLvjFMc-3"
      },
      "source": [
        "---\n",
        "## 2. Loading JSON files\n",
        "\n",
        "Many of the data in the Internet exists in JSON format which is a structured text format, and is very similar to a Python dictionary.\n",
        "\n",
        "We will see how to load a JSON dataset in a Pandas DataFrame.\n",
        "\n",
        "We will use the Citibike API that provides a real-time view of the Citibike stations in New York.\n",
        "The API call at http://www.citibikenyc.com/stations/json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdSOwaauvc3U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Display max 50 rows\n",
        "pd.set_option('display.max_rows', 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42hyYkTRvc3U"
      },
      "outputs": [],
      "source": [
        "# Read json file\n",
        "\n",
        "# import urllib library\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# import json\n",
        "import json\n",
        "# store the URL in url as\n",
        "# parameter for urlopen\n",
        "url = \"https://raw.githubusercontent.com/michalis0/Business-Intelligence-and-Analytics/Editing/data/station_status/station_status.json\"\n",
        "\n",
        "# store the response of URL\n",
        "response = urlopen(url)\n",
        "\n",
        "# storing the JSON response\n",
        "# from url in data\n",
        "data = json.loads(response.read())\n",
        "\n",
        "#uncomment to see how the data looks:\n",
        "#print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsuOO0WKvc3V"
      },
      "outputs": [],
      "source": [
        "# Below you see how the JSON file looks. The JSON results contain two keys:\n",
        "#  - The 'data' key contains all the data\n",
        "#  - The 'last_updated' key contains the timestamp of the last update\n",
        "#  - And the 'ttl' key contains the time to live of the data\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCiLDwz-vc3V"
      },
      "outputs": [],
      "source": [
        "# The field \"data\" contains the information we want but it has a nested structure\n",
        "data[\"data\"].keys()\n",
        "# The field \"stations\" contains the actual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZRE6sAZvc3V"
      },
      "outputs": [],
      "source": [
        "# We can convert the nested dictionary to a pandas dataframe\n",
        "# We want to extract the data from the key \"data\"\n",
        "df_info = pd.DataFrame(data[\"data\"][\"stations\"])\n",
        "df_info.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyTjJ6-5vc3W"
      },
      "outputs": [],
      "source": [
        "# Display types of each column\n",
        "df_info.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brOhicEjvc3W"
      },
      "outputs": [],
      "source": [
        "# Replace NAN value of all integer fields by 0\n",
        "df_info.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Grouping values by feature\n",
        "\n",
        "Sometimes if you want to get an indicator (e.g. mean, max, min...) on of the data on one particular value of a feature, you will first need to group values in relation to the values of that feature. To do so, we use a function in the Pandas library, `groupby()`. Let's say for example that we want to know the mean of available bikes at the renting stations (is_renting = 1). We have to group values in function of this feature."
      ],
      "metadata": {
        "id": "DIXS5eTbCdrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the mean of bikes available for renting stations (is_renting = 1) and non-renting stations (is_renting = 0)\n",
        "df_info_mean= df_info.groupby('is_renting')['num_bikes_available'].mean()\n",
        "df_info_mean.head()"
      ],
      "metadata": {
        "id": "NIL9NHRrCd1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wHkwCavvc3W"
      },
      "source": [
        "---\n",
        "For the following questions, you may find usefull to use the following functions:\n",
        "- `describe()`,\n",
        "- `mean()`,\n",
        "- `groupby()`,\n",
        "- `where()`,\n",
        "- `sort_values()`\n",
        "\n",
        "You will find more information about them on the offcial documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts8TUGJvvc3W"
      },
      "source": [
        "__Question 2.1:__ Add a column nammed `ebike_prop` which contains the proportion of available e-bikes among all available bikes. That is we want the following ratio expressed in percenatge : $$\\frac{num \\ ebikes \\ available}{num \\ ebikes \\ available + num \\ bikes \\ available}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgGyrdvmvc3X"
      },
      "outputs": [],
      "source": [
        "# Add column with the proportion of e-bikes available\n",
        "df_info[\"ebike_prop\"] = ... # enter your solution here\n",
        "df_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mZ3r3yrvc3X"
      },
      "source": [
        "__Question 2.2:__ What are the `station_id` of the stations with the greatest proportion of available e-bikes ? Display the first 5 stations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuvIgy8Ivc3Y"
      },
      "outputs": [],
      "source": [
        "# enter your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKixlJIZvc3Y"
      },
      "source": [
        "__Question 2.3:__ What are the `station_id` of the stations with the greatest proportion of available e-bikes where the number of available e-bikes is greater or equal to 10 (so that you can maximize your chances to get an e-bike by the time you reach the station) ? Display the first 10 stations id with the proportion, number of docks of the station and the number of bikes available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "378ph2E7vc3Z"
      },
      "outputs": [],
      "source": [
        "# enter your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUPJVJiLvc3Z"
      },
      "source": [
        "__Question 2.4:__ It is often interesting to find simple statistics about a column like the mean, or the highest values. Whhat is the proportion of stations with a proportion of available e-bikes >= 50 % ? Same question for a proportion of 75 %."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywmQVyqYvc3Z"
      },
      "outputs": [],
      "source": [
        "# Display the statistics of the initial data file for the newly_created column \"ebike_prop\"\n",
        "# enter your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez35041svc3a"
      },
      "source": [
        "__Question 2.5:__ Print the number of disabled bikes in the entire network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bm5UcTavc3a"
      },
      "outputs": [],
      "source": [
        "# enter your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZhwxyTjvc3a"
      },
      "source": [
        "__Question 2.6:__ Count the number of stations with at least 5 bikes unavailable. Display the following fields:\n",
        "- station_id,\n",
        "- station_status,\n",
        "- num_bikes_available,\n",
        "- num_docks_available,\n",
        "- num_bikes_disabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESYgL8A7Mc_P"
      },
      "source": [
        "---\n",
        "## Writing the data to a CSV\n",
        "\n",
        "With the above, we just scratched the surface of what it means to do data processing.\n",
        "\n",
        "After you did your basic data processing, you may want to save the DataFrame in a new CSV file, so that you don't have to repeat the same pre-processing everytime. You can use the [to_csv](https://datatofish.com/export-dataframe-to-csv/) function.\n",
        "\n",
        "**Note**: When you use Google Colab, this file will only be saved in your temporary virtual machine space and will be deleted once your Colab instance is closed (i.e. you close the window). If you want to explore more permanent solutions of saving your file look [here](https://colab.research.google.com/notebooks/io.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geghC3KjMc_Q"
      },
      "outputs": [],
      "source": [
        "# uncomment the following to save the file\n",
        "# df_info.to_csv(\"my_new_file.csv\", sep=\",\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RJYHORgqMc-x"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0cc462c96df3621bcc58e01fadcdf9264a069c5c4bbf07201077bb349d3c6bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}